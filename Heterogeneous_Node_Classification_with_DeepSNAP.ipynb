{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXWJLEm2UWS"
      },
      "source": [
        "# **Heterogeneous Node Classification with DeepSNAP**\n",
        "\n",
        "Heterogeneous graphs extend the traditional homogenous graphs by specifically incorperating different node and edge types. This additional information allows us to extend the traditional graph neural nework models, such as applying the heterogenous message passing, where different message types now exist between different node, edge type relationships.\n",
        "\n",
        "In this tutorial, we will build a heterogenous graph neural netowrk model by using [PyTorch Geonetric](https://pytorch-geometric.readthedocs.io/en/latest/) and [DeepSNAP](https://snap.stanford.edu/deepsnap/) on the heterogeneous node property prediction (node classification) task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSaetj53YnT6"
      },
      "source": [
        "# Device\n",
        "\n",
        "Please click `Runtime` and then `Change runtime type`. Then set the `hardware accelerator` to **GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67gOQITlCNQi"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (2.5.1)\n",
            "Requirement already satisfied: filelock in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/nathanzh/miniconda3/envs/deepsnap/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_m9l6OYCQZP",
        "outputId": "1c1f4425-79c9-4b1a-86ce-a7e3e2cce4d9"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5LsVSRuI3hU"
      },
      "source": [
        "# Heterogeneous Graph Node Classification\n",
        "\n",
        "In this tutorial, we will use PyTorch Geometric and DeepSNAP to implement a GNN model for heterogeneous graph node property prediction (node classification).\n",
        "\n",
        "At first let's take look at the general structure of a heterogeneous layer by an example.\n",
        "\n",
        "Let's assume we have a graph $G$, which contains two node types $a$ and $b$, and three message types $m_1=(a, r_1, a)$, $m_2=(a, r_2, b)$ and $m_3=(a, r_3, b)$.\n",
        "\n",
        "Thus, for $G$ a heterogeneous layer will contains three Heterogeneous GNN layers (`HeteroGNNConv` in this Colab) where each `HeteroGNNConv` layer will perform the message passing and aggregation with respect to only one message type. The overview of the heterogeneous layer is shown below:\n",
        "\n",
        "<br/>\n",
        "<center>\n",
        "<img src=\"https://web.stanford.edu/class/cs224w/images/colab4/hetero_conv.png\"/>\n",
        "</center>\n",
        "<br/>\n",
        "\n",
        "In this Colab, all the $l^{th}$ Heterogeneous GNN layers will be managed by a ($l^{th}$) Heterogeneous GNN Wrapper layer (the `HeteroGNNWrapperConv`). The $l^{th}$ Heterogeneous GNN Wrapper layer will take in the input node embeddings from $(l-1)^{th}$ layer and aggregate (across message types) the Heterogeneous GNN layers' results. For example, the wrapper layer will aggregate node type $b$'s node embeddings from Heterogeneous GNN layers for $m_2$ and $m_3$. The \"simplified\" heterogeneous layer structure is shown below:\n",
        "\n",
        "<br/>\n",
        "<center>\n",
        "<img src=\"http://web.stanford.edu/class/cs224w/images/colab4/hetero_conv_1.png\"/>\n",
        "</center>\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkFjcktiJJLm"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NAm9_OcJJJ-W"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import deepsnap\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.nn as pyg_nn\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from deepsnap.hetero_gnn import forward_op\n",
        "from deepsnap.hetero_graph import HeteroGraph\n",
        "from torch_sparse import SparseTensor, matmul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBlboS5kJmJL"
      },
      "source": [
        "## Heterogeneous GNN Layer\n",
        "\n",
        "Now let's start working on our own implementation of a heterogeneous layer (the `HeteroGNNConv`)! In general, our heterogeneous GNN layer draws ideas from the **GraphSAGE** ([Hamilton et al. (2017)](https://arxiv.org/abs/1706.02216)).\n",
        "\n",
        "At first, let's implement the GNN layer for each message type:\n",
        "\n",
        "\\begin{equation}\n",
        "m =(s, r, d)\n",
        "\\end{equation}\n",
        "\n",
        "Each message type is a tuple containing three elements where $s$ refers to the source node type, $r$ refers to the edge (relation) type and $d$ refers to the destination node type. The update rule is very similar to that of GraphSAGE but we need to include the node types and the edge type. The update rule is described as below:\n",
        "\n",
        "\\begin{equation}\n",
        "h_v^{(l)[m]} = W^{(l)[m]} \\cdot \\text{CONCAT} \\Big( W_d^{(l)[m]} \\cdot h_v^{(l-1)}, W_s^{(l)[m]} \\cdot AGG(\\{h_u^{(l-1)}, \\forall u \\in N_{m}(v) \\})\\Big)\n",
        "\\end{equation}\n",
        "\n",
        "where $[m]$ indicates that the weight matrices or embeddings with respect to message type $m$, $W_s^{(l)[m]}$ computes the messages from neighboring nodes, $W_d^{(l)[m]}$ compute messages from the node itself, and $W^{(l)[m]}$ aggregates messages from both node types. In the equation above, $v$ has the node type $d$, and $u$ has the node type $s$.\n",
        "\n",
        "For simplicity, we use mean aggregations for $AGG$ where:\n",
        "\n",
        "\\begin{equation}\n",
        "AGG(\\{h_u^{(l-1)}, \\forall u \\in N_{m}(v) \\}) = \\frac{1}{|N_{m}(v)|} \\sum_{u\\in N_{m}(v)} h_u^{(l-1)}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_Z1b0Mf8Jova"
      },
      "outputs": [],
      "source": [
        "class HeteroGNNConv(pyg_nn.MessagePassing):\n",
        "    def __init__(self, in_channels_src, in_channels_dst, out_channels):\n",
        "        super(HeteroGNNConv, self).__init__(aggr=\"mean\")\n",
        "\n",
        "        self.in_channels_src = in_channels_src\n",
        "        self.in_channels_dst = in_channels_dst\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        self.lin_src = nn.Linear(self.in_channels_src, self.out_channels)\n",
        "        self.lin_dst = nn.Linear(self.in_channels_dst, self.out_channels)\n",
        "        self.lin_update = nn.Linear(self.out_channels * 2, self.out_channels)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        node_feature_src,\n",
        "        node_feature_dst,\n",
        "        edge_index,\n",
        "        size=None\n",
        "    ):\n",
        "        return self.propagate(\n",
        "            edge_index, size=size,\n",
        "            node_feature_dst=node_feature_dst,\n",
        "            node_feature_src=node_feature_src\n",
        "        )\n",
        "\n",
        "    def message_and_aggregate(self, edge_index, node_feature_src):\n",
        "        out = matmul(edge_index, node_feature_src, reduce=\"mean\")\n",
        "        return out\n",
        "\n",
        "    def update(self, aggr_out, node_feature_dst):\n",
        "        aggr_out = self.lin_src(aggr_out)\n",
        "        node_feature_dst = self.lin_dst(node_feature_dst)\n",
        "        aggr_out = torch.cat([aggr_out, node_feature_dst], dim=-1)\n",
        "        aggr_out = self.lin_update(aggr_out)\n",
        "        return aggr_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKq8ScTiJthn"
      },
      "source": [
        "## Heterogeneous GNN Wrapper Layer\n",
        "\n",
        "After implementing the GNN layer for each message type, we need to aggregate the the node embedding results (with respect to each message types) together. Here we will implement two types of message type level aggregation.\n",
        "\n",
        "The first one is simply the mean aggregation:\n",
        "\n",
        "\\begin{equation}\n",
        "h_v^{(l)} = \\frac{1}{M}\\sum_{m=1}^{M}h_v^{(l)[m]}\n",
        "\\end{equation}\n",
        "\n",
        "Here node $v$ has the node type $d$ and $M$ is the total number of message types that the destination node type is $d$.\n",
        "\n",
        "The other one is the semantic level attention introduced in **HAN** ([Wang et al. (2019)](https://arxiv.org/abs/1903.07293)). Instead of directly averaging on the message type aggregation results, we can use attention to learn which message type result can be more important, then aggregate from all the message types. Following are the equations for semantic level attention:\n",
        "\n",
        "\\begin{equation}\n",
        "e_{m} = \\frac{1}{|V_{d}|} \\sum_{v \\in V_{d}} q_{attn}^T \\cdot tanh \\Big( W_{attn}^{(l)} \\cdot h_v^{(l)[m]} + b \\Big)\n",
        "\\end{equation}\n",
        "\n",
        "where $m$ refers to message type and $d$ refers to the destination node type. Then we can compute the attention and update the $h_v^{(l)}$:\n",
        "\n",
        "\\begin{equation}\n",
        "\\alpha_{m} = \\frac{\\exp(e_{m})}{\\sum_{m=1}^M \\exp(e_{m})}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "h_v^{(l)} = \\sum_{m=1}^{M} \\alpha_{m} \\cdot h_v^{(l)[m]}\n",
        "\\end{equation}\n",
        "\n",
        "**Notice**: You can directly use `deepsnap.hetero_gnn.HeteroConv` directly for the mean aggregation. Here we overide the `HeteroConv` in order to support the semantic level attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0_bun02xJwFm"
      },
      "outputs": [],
      "source": [
        "class HeteroGNNWrapperConv(deepsnap.hetero_gnn.HeteroConv):\n",
        "    def __init__(self, convs, args, aggr=\"mean\"):\n",
        "        super(HeteroGNNWrapperConv, self).__init__(convs, None)\n",
        "        self.aggr = aggr\n",
        "\n",
        "        # Map the index and message type\n",
        "        self.mapping = {}\n",
        "\n",
        "        # A numpy array that stores the final attention probability\n",
        "        self.alpha = None\n",
        "\n",
        "        if self.aggr == \"attn\":\n",
        "            self.attn_proj = nn.Sequential(\n",
        "                nn.Linear(args['hidden_size'], args['attn_size']),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(args['attn_size'], 1, bias=False)\n",
        "            )\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        super(HeteroConvWrapper, self).reset_parameters()\n",
        "        if self.aggr == \"attn\":\n",
        "            for layer in self.attn_proj.children():\n",
        "                layer.reset_parameters()\n",
        "\n",
        "    def forward(self, node_features, edge_indices):\n",
        "        message_type_emb = {}\n",
        "        for message_key, message_type in edge_indices.items():\n",
        "            src_type, edge_type, dst_type = message_key\n",
        "            node_feature_src = node_features[src_type]\n",
        "            node_feature_dst = node_features[dst_type]\n",
        "            edge_index = edge_indices[message_key]\n",
        "            message_type_emb[message_key] = (\n",
        "                self.convs[message_key](\n",
        "                    node_feature_src,\n",
        "                    node_feature_dst,\n",
        "                    edge_index,\n",
        "                )\n",
        "            )\n",
        "        node_emb = {dst: [] for _, _, dst in message_type_emb.keys()}\n",
        "        mapping = {}\n",
        "        for (src, edge_type, dst), item in message_type_emb.items():\n",
        "            mapping[len(node_emb[dst])] = (src, edge_type, dst)\n",
        "            node_emb[dst].append(item)\n",
        "        self.mapping = mapping\n",
        "        for node_type, embs in node_emb.items():\n",
        "            if len(embs) == 1:\n",
        "                node_emb[node_type] = embs[0]\n",
        "            else:\n",
        "                node_emb[node_type] = self.aggregate(embs)\n",
        "        return node_emb\n",
        "\n",
        "    def aggregate(self, xs):\n",
        "        if self.aggr == \"mean\":\n",
        "            x = torch.stack(xs, dim=-1)\n",
        "            return x.mean(dim=-1)\n",
        "        elif self.aggr == \"attn\":\n",
        "            N = xs[0].shape[0] # Number of nodes for that node type\n",
        "            M = len(xs) # Number of message types for that node type\n",
        "\n",
        "            x = torch.cat(xs, dim=0).view(M, N, -1) # M * N * D\n",
        "            z = self.attn_proj(x).view(M, N) # M * N * 1\n",
        "            z = z.mean(1) # M * 1\n",
        "            alpha = torch.softmax(z, dim=0) # M * 1\n",
        "\n",
        "            # Store the attention result to self.alpha as np array\n",
        "            self.alpha = alpha.view(-1).data.cpu().numpy()\n",
        "\n",
        "            alpha = alpha.view(M, 1, 1)\n",
        "            x = x * alpha\n",
        "            return x.sum(dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn_pnCOKJw-d"
      },
      "source": [
        "## Initialize Heterogeneous GNN Layers\n",
        "\n",
        "Now let's initialize the Heterogeneous GNN Layers. Different from homogeneous graph case, heterogeneous case can be a little bit complex.\n",
        "\n",
        "In general, we need to create a dictionary of `HeteroGNNConv` layers where the keys are message types.\n",
        "\n",
        "* To get all message types, we can use `deepsnap.hetero_graph.HeteroGraph.message_types`.\n",
        "* If we are initializing the first conv layers, we need to get the feature dimension of each node type. For this we can use `deepsnap.hetero_graph.HeteroGraph.num_node_features(node_type)` which will return the node feature dimension of `node_type`. In this function, we set each `HeteroGNNConv` `out_channels` to be `hidden_size`.\n",
        "* If we are not initializing the first conv layers, all node types will have the same embedding dimension `hidden_size` and we still set `HeteroGNNConv` `out_channels` to be `hidden_size` for simplicity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hSBImHClJzf4"
      },
      "outputs": [],
      "source": [
        "def generate_convs(hetero_graph, conv, hidden_size, first_layer=False):\n",
        "    convs = {}\n",
        "    for message_type in hetero_graph.message_types:\n",
        "        if first_layer is True:\n",
        "            src_type = message_type[0]\n",
        "            dst_type = message_type[2]\n",
        "            src_size = hetero_graph.num_node_features(src_type)\n",
        "            dst_size = hetero_graph.num_node_features(dst_type)\n",
        "            convs[message_type] = conv(src_size, dst_size, hidden_size)\n",
        "        else:\n",
        "            convs[message_type] = conv(hidden_size, hidden_size, hidden_size)\n",
        "    return convs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U39dX8EpJ3FG"
      },
      "source": [
        "## HeteroGNN\n",
        "\n",
        "Now we will make a simple HeteroGNN model which contains only two `HeteroGNNWrapperConv` layers.\n",
        "\n",
        "For the forward function in `HeteroGNN`, the model is going to be run as following:\n",
        "\n",
        "$\\text{self.convs1} \\rightarrow \\text{self.bns1} \\rightarrow \\text{self.relus1} \\rightarrow \\text{self.convs2} \\rightarrow \\text{self.bns2} \\rightarrow \\text{self.relus2} \\rightarrow \\text{self.post_mps}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rplknA8aJ6J5"
      },
      "outputs": [],
      "source": [
        "class HeteroGNN(torch.nn.Module):\n",
        "    def __init__(self, hetero_graph, args, aggr=\"mean\"):\n",
        "        super(HeteroGNN, self).__init__()\n",
        "\n",
        "        self.aggr = aggr\n",
        "        self.hidden_size = args['hidden_size']\n",
        "\n",
        "        self.bns1 = nn.ModuleDict()\n",
        "        self.bns2 = nn.ModuleDict()\n",
        "        self.relus1 = nn.ModuleDict()\n",
        "        self.relus2 = nn.ModuleDict()\n",
        "        self.post_mps = nn.ModuleDict()\n",
        "\n",
        "        convs1 = generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size, first_layer=True)\n",
        "        convs2 = generate_convs(hetero_graph, HeteroGNNConv, self.hidden_size)\n",
        "\n",
        "        self.convs1 = HeteroGNNWrapperConv(convs1, args, aggr=self.aggr)\n",
        "        self.convs2 = HeteroGNNWrapperConv(convs2, args, aggr=self.aggr)\n",
        "\n",
        "        for node_type in hetero_graph.node_types:\n",
        "            self.bns1[node_type] = torch.nn.BatchNorm1d(self.hidden_size, eps=args['eps'])\n",
        "            self.bns2[node_type] = torch.nn.BatchNorm1d(self.hidden_size, eps=args['eps'])\n",
        "            self.post_mps[node_type] = nn.Linear(self.hidden_size, hetero_graph.num_node_labels(node_type))\n",
        "            self.relus1[node_type] = nn.LeakyReLU()\n",
        "            self.relus2[node_type] = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, node_feature, edge_index):\n",
        "        x = node_feature\n",
        "        x = self.convs1(x, edge_index)\n",
        "        x = forward_op(x, self.bns1)\n",
        "        x = forward_op(x, self.relus1)\n",
        "        x = self.convs2(x, edge_index)\n",
        "        x = forward_op(x, self.bns2)\n",
        "        x = forward_op(x, self.relus2)\n",
        "        x = forward_op(x, self.post_mps)\n",
        "        return x\n",
        "\n",
        "    def loss(self, preds, y, indices):\n",
        "        loss = 0\n",
        "        loss_func = F.cross_entropy\n",
        "        for node_type in preds:\n",
        "            idx = indices[node_type]\n",
        "            loss += loss_func(preds[node_type][idx], y[node_type][idx])\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9e7q_hUJ8zB"
      },
      "source": [
        "## Training and Testing\n",
        "\n",
        "Here are the functions to train and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CI5Hl_5TJ_YL"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, hetero_graph, train_idx):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    preds = model(hetero_graph.node_feature, hetero_graph.edge_index)\n",
        "\n",
        "    loss = model.loss(preds, hetero_graph.node_label, train_idx)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def test(model, graph, indices, best_model=None, best_val=0):\n",
        "    model.eval()\n",
        "    accs = []\n",
        "    for index in indices:\n",
        "        preds = model(graph.node_feature, graph.edge_index)\n",
        "        num_node_types = 0\n",
        "        micro = 0\n",
        "        macro = 0\n",
        "        for node_type in preds:\n",
        "            idx = index[node_type]\n",
        "            pred = preds[node_type][idx]\n",
        "            pred = pred.max(1)[1]\n",
        "            label_np = graph.node_label[node_type][idx].cpu().numpy()\n",
        "            pred_np = pred.cpu().numpy()\n",
        "            micro = f1_score(label_np, pred_np, average='micro')\n",
        "            macro = f1_score(label_np, pred_np, average='macro')\n",
        "            num_node_types += 1\n",
        "        # Averaging f1 score might not make sense, but in our example we only\n",
        "        # have one node type\n",
        "        micro /= num_node_types\n",
        "        macro /= num_node_types\n",
        "        accs.append((micro, macro))\n",
        "    if accs[1][0] > best_val:\n",
        "        best_val = accs[1][0]\n",
        "        best_model = copy.deepcopy(model)\n",
        "    return accs, best_model, best_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "DpNz9B5AKBUU"
      },
      "outputs": [],
      "source": [
        "args = {\n",
        "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    'hidden_size': 128,\n",
        "    'epochs': 100,\n",
        "    'weight_decay': 1e-5,\n",
        "    'lr': 0.003,\n",
        "    'attn_size': 128,\n",
        "    'eps': 1.0,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRHbWD4hKED8"
      },
      "source": [
        "## Dataset and Preprocessing\n",
        "\n",
        "In the next, we will load the data and create a tensor backend (without a NetworkX graph) `deepsnap.hetero_graph.HeteroGraph` object.\n",
        "\n",
        "We will use the `ACM(3025)` dataset in our node property prediction task, which is proposed in **HAN** ([Wang et al. (2019)](https://arxiv.org/abs/1903.07293)) and our dataset is extracted from [DGL](https://www.dgl.ai/)'s [ACM.mat](https://data.dgl.ai/dataset/ACM.mat).\n",
        "\n",
        "Now, let's download the extracted dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc-1DuedNwFj"
      },
      "source": [
        "The original ACM dataset has three node types and two edge (relation) types. For simplicity, we simplify the heterogeneous graph to one node type and two edge types (shown below). This means that in our heterogeneous graph, we have one node type (paper) and two message types *(paper, author, paper)* and *(paper, subject, paper)*.\n",
        "\n",
        "<br/>\n",
        "<center>\n",
        "<img src=\"http://web.stanford.edu/class/cs224w/images/colab4/cs224w-acm.png\"/>\n",
        "</center>\n",
        "\n",
        "Following is the code for dataset preprocessing. Here for efficiency, we only use the tensor backend for the DeepSNAP `HeteroGraph`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJy03_IsKGh6",
        "outputId": "1b7026b4-9b8b-48e2-a1cb-3b2c35661d22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "ACM heterogeneous graph: {'paper': 1096} nodes, {('paper', 'author', 'paper'): 7456, ('paper', 'subject', 'paper'): 12496} edges\n",
            "SparseTensor(row=tensor([   0,    0,    0,  ..., 1095, 1095, 1095], device='cuda:0'),\n",
            "             col=tensor([   0,  486,  851,  ...,  670, 1052, 1095], device='cuda:0'),\n",
            "             size=(1096, 1096), nnz=7456, density=0.62%)\n",
            "SparseTensor(row=tensor([   0,    1,    2,  ..., 1095, 1095, 1095], device='cuda:0'),\n",
            "             col=tensor([   0,    1,    2,  ...,  186,  586, 1095], device='cuda:0'),\n",
            "             size=(1096, 1096), nnz=12496, density=1.04%)\n"
          ]
        }
      ],
      "source": [
        "print(\"Device: {}\".format(args['device']))\n",
        "\n",
        "import pickle \n",
        "\n",
        "# Load the data\n",
        "# with open(\"processed_graphs_combined/combined_conferences.pkl\", 'rb') as f:\n",
        "#     data = pickle.load(f)\n",
        "\n",
        "# Load American data\n",
        "# with open(\"processed_graphs_combined/combined_conferences_american.pkl\", 'rb') as f:\n",
        "#     data = pickle.load(f)\n",
        "\n",
        "# Load USENIX data\n",
        "# with open(\"processed_usenix/usenix_processed.pkl\", 'rb') as f:\n",
        "#     data = pickle.load(f)\n",
        "\n",
        "# Load USENIX American data\n",
        "with open(\"processed_usenix/usenix_processed_american.pkl\", 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# Original data\n",
        "# data = torch.load(\"acm.pkl\")\n",
        "\n",
        "# Message types\n",
        "message_type_1 = (\"paper\", \"author\", \"paper\")\n",
        "message_type_2 = (\"paper\", \"subject\", \"paper\")\n",
        "\n",
        "# Dictionary of edge indices\n",
        "edge_index = {}\n",
        "edge_index[message_type_1] = data['pap']\n",
        "edge_index[message_type_2] = data['psp']\n",
        "\n",
        "# Dictionary of node features\n",
        "node_feature = {}\n",
        "node_feature[\"paper\"] = data['feature']\n",
        "\n",
        "# Dictionary of node labels\n",
        "node_label = {}\n",
        "node_label[\"paper\"] = data['label']\n",
        "\n",
        "# Load the train, validation and test indices\n",
        "train_idx = {\"paper\": data['train_idx'].to(args['device'])}\n",
        "val_idx = {\"paper\": data['val_idx'].to(args['device'])}\n",
        "test_idx = {\"paper\": data['test_idx'].to(args['device'])}\n",
        "\n",
        "# Construct a deepsnap tensor backend HeteroGraph\n",
        "hetero_graph = HeteroGraph(\n",
        "    node_feature=node_feature,\n",
        "    node_label=node_label,\n",
        "    edge_index=edge_index,\n",
        "    directed=True\n",
        ")\n",
        "\n",
        "print(f\"ACM heterogeneous graph: {hetero_graph.num_nodes()} nodes, {hetero_graph.num_edges()} edges\")\n",
        "\n",
        "# Node feature and node label to device\n",
        "for key in hetero_graph.node_feature:\n",
        "    hetero_graph.node_feature[key] = hetero_graph.node_feature[key].to(args['device'])\n",
        "for key in hetero_graph.node_label:\n",
        "    hetero_graph.node_label[key] = hetero_graph.node_label[key].to(args['device'])\n",
        "\n",
        "# Edge_index to sparse tensor and to device\n",
        "for key in hetero_graph.edge_index:\n",
        "    edge_index = hetero_graph.edge_index[key]\n",
        "    adj = SparseTensor(row=edge_index[0], col=edge_index[1], sparse_sizes=(hetero_graph.num_nodes('paper'), hetero_graph.num_nodes('paper')))\n",
        "    hetero_graph.edge_index[key] = adj.t().to(args['device'])\n",
        "print(hetero_graph.edge_index[message_type_1])\n",
        "print(hetero_graph.edge_index[message_type_2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrmU5-QQKJv6"
      },
      "source": [
        "## Start Training!\n",
        "\n",
        "Now lets start training!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0HplV9hKMkc"
      },
      "source": [
        "## Training the Mean Aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgwfyzLbKOUw",
        "outputId": "f0f2c883-2be5-4bf6-849e-33c98cf02e73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: loss 0.67758, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 2: loss 0.66844, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 3: loss 0.65596, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 4: loss 0.63545, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 5: loss 0.59859, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 6: loss 0.53524, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 7: loss 0.44551, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 8: loss 0.35895, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: loss 0.31134, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 10: loss 0.29298, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 11: loss 0.2801, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 12: loss 0.26627, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 13: loss 0.2531, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 14: loss 0.24332, train micro 93.48%, train macro 48.32%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 15: loss 0.23709, train micro 93.48%, train macro 52.01%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 16: loss 0.23102, train micro 93.35%, train macro 51.91%, valid micro 91.46%, valid macro 47.77%, test micro 92.12%, test macro 47.95%\n",
            "Epoch 17: loss 0.22481, train micro 93.22%, train macro 51.81%, valid micro 91.46%, valid macro 47.77%, test micro 92.73%, test macro 48.11%\n",
            "Epoch 18: loss 0.22039, train micro 93.22%, train macro 50.1%, valid micro 91.46%, valid macro 47.77%, test micro 92.12%, test macro 47.95%\n",
            "Epoch 19: loss 0.21798, train micro 93.35%, train macro 50.16%, valid micro 90.85%, valid macro 47.6%, test micro 91.52%, test macro 47.78%\n",
            "Epoch 20: loss 0.21638, train micro 93.35%, train macro 50.16%, valid micro 90.85%, valid macro 47.6%, test micro 92.12%, test macro 47.95%\n",
            "Epoch 21: loss 0.21437, train micro 93.48%, train macro 50.24%, valid micro 90.85%, valid macro 47.6%, test micro 92.73%, test macro 48.11%\n",
            "Epoch 22: loss 0.21135, train micro 93.48%, train macro 50.24%, valid micro 91.46%, valid macro 47.77%, test micro 92.12%, test macro 47.95%\n",
            "Epoch 23: loss 0.20762, train micro 93.61%, train macro 52.12%, valid micro 91.46%, valid macro 47.77%, test micro 92.73%, test macro 48.11%\n",
            "Epoch 24: loss 0.20357, train micro 93.48%, train macro 52.01%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 25: loss 0.19971, train micro 93.48%, train macro 52.01%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 26: loss 0.19608, train micro 93.74%, train macro 50.38%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 27: loss 0.19206, train micro 94.0%, train macro 50.54%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 28: loss 0.18729, train micro 94.0%, train macro 50.54%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 29: loss 0.18211, train micro 94.0%, train macro 50.54%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 30: loss 0.17692, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 31: loss 0.17126, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 32: loss 0.16544, train micro 94.0%, train macro 50.54%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 33: loss 0.1592, train micro 94.13%, train macro 52.57%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 34: loss 0.15248, train micro 94.26%, train macro 54.52%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 35: loss 0.14564, train micro 94.26%, train macro 54.52%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 36: loss 0.13839, train micro 94.26%, train macro 54.52%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 37: loss 0.13, train micro 94.39%, train macro 56.39%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 38: loss 0.12063, train micro 94.26%, train macro 54.52%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 39: loss 0.11058, train micro 94.52%, train macro 58.2%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 40: loss 0.10034, train micro 94.78%, train macro 61.61%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 41: loss 0.09068, train micro 95.05%, train macro 64.79%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 42: loss 0.08204, train micro 95.05%, train macro 64.79%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 43: loss 0.07351, train micro 95.44%, train macro 69.15%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 44: loss 0.0651, train micro 95.96%, train macro 74.34%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 45: loss 0.0573, train micro 96.87%, train macro 82.04%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 46: loss 0.05023, train micro 97.13%, train macro 83.97%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 47: loss 0.04379, train micro 97.78%, train macro 88.38%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 48: loss 0.03767, train micro 98.57%, train macro 92.99%, valid micro 90.85%, valid macro 47.6%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 49: loss 0.03189, train micro 99.35%, train macro 97.02%, valid micro 90.85%, valid macro 47.6%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 50: loss 0.02685, train micro 99.48%, train macro 97.64%, valid micro 90.24%, valid macro 47.44%, test micro 92.73%, test macro 48.11%\n",
            "Epoch 51: loss 0.02269, train micro 99.48%, train macro 97.64%, valid micro 88.41%, valid macro 46.93%, test micro 93.33%, test macro 55.96%\n",
            "Epoch 52: loss 0.01917, train micro 99.61%, train macro 98.25%, valid micro 87.2%, valid macro 46.58%, test micro 91.52%, test macro 54.02%\n",
            "Epoch 53: loss 0.01588, train micro 99.87%, train macro 99.43%, valid micro 86.59%, valid macro 46.41%, test micro 90.91%, test macro 58.11%\n",
            "Epoch 54: loss 0.01327, train micro 99.87%, train macro 99.43%, valid micro 87.2%, valid macro 50.91%, test micro 90.91%, test macro 61.86%\n",
            "Epoch 55: loss 0.0117, train micro 99.87%, train macro 99.43%, valid micro 86.59%, valid macro 50.55%, test micro 89.7%, test macro 60.27%\n",
            "Epoch 56: loss 0.01013, train micro 100.0%, train macro 100.0%, valid micro 85.37%, valid macro 49.87%, test micro 87.27%, test macro 57.65%\n",
            "Epoch 57: loss 0.0085, train micro 100.0%, train macro 100.0%, valid micro 84.15%, valid macro 49.24%, test micro 87.27%, test macro 57.65%\n",
            "Epoch 58: loss 0.00696, train micro 100.0%, train macro 100.0%, valid micro 84.15%, valid macro 49.24%, test micro 87.88%, test macro 58.25%\n",
            "Epoch 59: loss 0.00571, train micro 100.0%, train macro 100.0%, valid micro 83.54%, valid macro 48.93%, test micro 87.27%, test macro 57.65%\n",
            "Epoch 60: loss 0.00474, train micro 100.0%, train macro 100.0%, valid micro 82.32%, valid macro 48.34%, test micro 87.27%, test macro 57.65%\n",
            "Epoch 61: loss 0.00401, train micro 100.0%, train macro 100.0%, valid micro 82.32%, valid macro 48.34%, test micro 87.88%, test macro 58.25%\n",
            "Epoch 62: loss 0.00345, train micro 100.0%, train macro 100.0%, valid micro 82.32%, valid macro 48.34%, test micro 88.48%, test macro 58.89%\n",
            "Epoch 63: loss 0.00304, train micro 100.0%, train macro 100.0%, valid micro 82.32%, valid macro 48.34%, test micro 88.48%, test macro 58.89%\n",
            "Epoch 64: loss 0.00271, train micro 100.0%, train macro 100.0%, valid micro 82.93%, valid macro 51.52%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 65: loss 0.00244, train micro 100.0%, train macro 100.0%, valid micro 84.15%, valid macro 52.3%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 66: loss 0.0022, train micro 100.0%, train macro 100.0%, valid micro 84.15%, valid macro 52.3%, test micro 89.7%, test macro 60.27%\n",
            "Epoch 67: loss 0.00199, train micro 100.0%, train macro 100.0%, valid micro 84.15%, valid macro 52.3%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 68: loss 0.0018, train micro 100.0%, train macro 100.0%, valid micro 84.76%, valid macro 52.72%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 69: loss 0.00163, train micro 100.0%, train macro 100.0%, valid micro 84.76%, valid macro 52.72%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 70: loss 0.00148, train micro 100.0%, train macro 100.0%, valid micro 84.76%, valid macro 52.72%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 71: loss 0.00134, train micro 100.0%, train macro 100.0%, valid micro 84.76%, valid macro 52.72%, test micro 89.7%, test macro 60.27%\n",
            "Epoch 72: loss 0.00122, train micro 100.0%, train macro 100.0%, valid micro 85.37%, valid macro 53.14%, test micro 90.3%, test macro 61.04%\n",
            "Epoch 73: loss 0.00111, train micro 100.0%, train macro 100.0%, valid micro 85.37%, valid macro 53.14%, test micro 90.3%, test macro 61.04%\n",
            "Epoch 74: loss 0.00102, train micro 100.0%, train macro 100.0%, valid micro 85.98%, valid macro 53.59%, test micro 90.3%, test macro 61.04%\n",
            "Epoch 75: loss 0.00093, train micro 100.0%, train macro 100.0%, valid micro 85.98%, valid macro 53.59%, test micro 90.3%, test macro 61.04%\n",
            "Epoch 76: loss 0.00086, train micro 100.0%, train macro 100.0%, valid micro 85.98%, valid macro 53.59%, test micro 90.91%, test macro 61.86%\n",
            "Epoch 77: loss 0.0008, train micro 100.0%, train macro 100.0%, valid micro 85.98%, valid macro 53.59%, test micro 90.91%, test macro 61.86%\n",
            "Epoch 78: loss 0.00074, train micro 100.0%, train macro 100.0%, valid micro 85.98%, valid macro 53.59%, test micro 91.52%, test macro 62.74%\n",
            "Epoch 79: loss 0.0007, train micro 100.0%, train macro 100.0%, valid micro 84.15%, valid macro 52.3%, test micro 92.12%, test macro 66.94%\n",
            "Epoch 80: loss 0.00065, train micro 100.0%, train macro 100.0%, valid micro 84.15%, valid macro 52.3%, test micro 92.12%, test macro 66.94%\n",
            "Epoch 81: loss 0.00061, train micro 100.0%, train macro 100.0%, valid micro 83.54%, valid macro 51.91%, test micro 91.52%, test macro 65.91%\n",
            "Epoch 82: loss 0.00058, train micro 100.0%, train macro 100.0%, valid micro 83.54%, valid macro 51.91%, test micro 91.52%, test macro 65.91%\n",
            "Epoch 83: loss 0.00055, train micro 100.0%, train macro 100.0%, valid micro 83.54%, valid macro 51.91%, test micro 91.52%, test macro 65.91%\n",
            "Epoch 84: loss 0.00052, train micro 100.0%, train macro 100.0%, valid micro 82.93%, valid macro 51.52%, test micro 90.91%, test macro 64.95%\n",
            "Epoch 85: loss 0.00049, train micro 100.0%, train macro 100.0%, valid micro 82.93%, valid macro 51.52%, test micro 90.91%, test macro 64.95%\n",
            "Epoch 86: loss 0.00047, train micro 100.0%, train macro 100.0%, valid micro 82.93%, valid macro 51.52%, test micro 90.91%, test macro 64.95%\n",
            "Epoch 87: loss 0.00045, train micro 100.0%, train macro 100.0%, valid micro 82.32%, valid macro 51.15%, test micro 90.91%, test macro 64.95%\n",
            "Epoch 88: loss 0.00043, train micro 100.0%, train macro 100.0%, valid micro 81.71%, valid macro 50.78%, test micro 90.91%, test macro 64.95%\n",
            "Epoch 89: loss 0.00042, train micro 100.0%, train macro 100.0%, valid micro 81.71%, valid macro 50.78%, test micro 90.91%, test macro 64.95%\n",
            "Epoch 90: loss 0.0004, train micro 100.0%, train macro 100.0%, valid micro 81.71%, valid macro 50.78%, test micro 90.91%, test macro 64.95%\n",
            "Epoch 91: loss 0.00039, train micro 100.0%, train macro 100.0%, valid micro 81.71%, valid macro 50.78%, test micro 90.91%, test macro 64.95%\n",
            "Epoch 92: loss 0.00037, train micro 100.0%, train macro 100.0%, valid micro 81.71%, valid macro 50.78%, test micro 90.91%, test macro 64.95%\n",
            "Epoch 93: loss 0.00036, train micro 100.0%, train macro 100.0%, valid micro 81.71%, valid macro 50.78%, test micro 90.91%, test macro 64.95%\n",
            "Epoch 94: loss 0.00035, train micro 100.0%, train macro 100.0%, valid micro 81.1%, valid macro 50.42%, test micro 90.91%, test macro 64.95%\n",
            "Epoch 95: loss 0.00034, train micro 100.0%, train macro 100.0%, valid micro 81.1%, valid macro 50.42%, test micro 90.91%, test macro 64.95%\n",
            "Epoch 96: loss 0.00033, train micro 100.0%, train macro 100.0%, valid micro 81.1%, valid macro 50.42%, test micro 90.91%, test macro 64.95%\n",
            "Epoch 97: loss 0.00032, train micro 100.0%, train macro 100.0%, valid micro 81.1%, valid macro 50.42%, test micro 90.91%, test macro 64.95%\n",
            "Epoch 98: loss 0.00031, train micro 100.0%, train macro 100.0%, valid micro 81.1%, valid macro 50.42%, test micro 90.91%, test macro 64.95%\n",
            "Epoch 99: loss 0.00031, train micro 100.0%, train macro 100.0%, valid micro 81.1%, valid macro 50.42%, test micro 91.52%, test macro 65.91%\n",
            "Epoch 100: loss 0.0003, train micro 100.0%, train macro 100.0%, valid micro 81.71%, valid macro 50.78%, test micro 91.52%, test macro 65.91%\n",
            "Best model: train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n"
          ]
        }
      ],
      "source": [
        "best_model = None\n",
        "best_val = 0\n",
        "\n",
        "model = HeteroGNN(hetero_graph, args, aggr=\"mean\").to(args['device'])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
        "\n",
        "for epoch in range(args['epochs']):\n",
        "    loss = train(model, optimizer, hetero_graph, train_idx)\n",
        "    accs, best_model, best_val = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_val)\n",
        "    print(\n",
        "        f\"Epoch {epoch + 1}: loss {round(loss, 5)}, \"\n",
        "        f\"train micro {round(accs[0][0] * 100, 2)}%, train macro {round(accs[0][1] * 100, 2)}%, \"\n",
        "        f\"valid micro {round(accs[1][0] * 100, 2)}%, valid macro {round(accs[1][1] * 100, 2)}%, \"\n",
        "        f\"test micro {round(accs[2][0] * 100, 2)}%, test macro {round(accs[2][1] * 100, 2)}%\"\n",
        "    )\n",
        "best_accs, _, _ = test(best_model, hetero_graph, [train_idx, val_idx, test_idx])\n",
        "print(\n",
        "    f\"Best model: \"\n",
        "    f\"train micro {round(best_accs[0][0] * 100, 2)}%, train macro {round(best_accs[0][1] * 100, 2)}%, \"\n",
        "    f\"valid micro {round(best_accs[1][0] * 100, 2)}%, valid macro {round(best_accs[1][1] * 100, 2)}%, \"\n",
        "    f\"test micro {round(best_accs[2][0] * 100, 2)}%, test macro {round(best_accs[2][1] * 100, 2)}%\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBiYvwcuKd0z"
      },
      "source": [
        "## Training the Attention Aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6na5zyQKfvi",
        "outputId": "3b148a3d-e2f3-4d22-a6e4-726b1c1c071f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: loss 0.65567, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 2: loss 0.64616, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 3: loss 0.63274, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 4: loss 0.61004, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 5: loss 0.56836, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 6: loss 0.49821, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 7: loss 0.40383, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 8: loss 0.32224, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 9: loss 0.2816, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 10: loss 0.26775, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 11: loss 0.25992, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 12: loss 0.25217, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 13: loss 0.24409, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 14: loss 0.23652, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 15: loss 0.23, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 16: loss 0.22445, train micro 93.61%, train macro 48.35%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 17: loss 0.21937, train micro 93.61%, train macro 48.35%, valid micro 91.46%, valid macro 47.77%, test micro 92.12%, test macro 47.95%\n",
            "Epoch 18: loss 0.21427, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 19: loss 0.21023, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 20: loss 0.20697, train micro 93.74%, train macro 48.38%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 21: loss 0.2034, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 22: loss 0.19975, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 23: loss 0.1964, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 24: loss 0.19274, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 25: loss 0.18873, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 26: loss 0.18398, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 27: loss 0.17839, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 28: loss 0.1723, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 29: loss 0.16546, train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 30: loss 0.15814, train micro 94.0%, train macro 50.54%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 31: loss 0.15067, train micro 94.0%, train macro 50.54%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 32: loss 0.14383, train micro 94.52%, train macro 59.69%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 33: loss 0.1371, train micro 94.52%, train macro 59.69%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 34: loss 0.12973, train micro 94.52%, train macro 61.08%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n",
            "Epoch 35: loss 0.12204, train micro 95.57%, train macro 70.51%, valid micro 91.46%, valid macro 47.77%, test micro 92.73%, test macro 55.24%\n",
            "Epoch 36: loss 0.11349, train micro 95.83%, train macro 73.11%, valid micro 91.46%, valid macro 47.77%, test micro 92.12%, test macro 54.6%\n",
            "Epoch 37: loss 0.10435, train micro 96.74%, train macro 81.03%, valid micro 90.24%, valid macro 47.44%, test micro 92.12%, test macro 54.6%\n",
            "Epoch 38: loss 0.09572, train micro 97.0%, train macro 83.02%, valid micro 90.24%, valid macro 47.44%, test micro 92.12%, test macro 54.6%\n",
            "Epoch 39: loss 0.08779, train micro 97.13%, train macro 83.97%, valid micro 89.63%, valid macro 47.27%, test micro 92.12%, test macro 54.6%\n",
            "Epoch 40: loss 0.08061, train micro 97.39%, train macro 85.8%, valid micro 89.63%, valid macro 52.51%, test micro 91.52%, test macro 54.02%\n",
            "Epoch 41: loss 0.0741, train micro 97.52%, train macro 86.68%, valid micro 89.02%, valid macro 52.08%, test micro 90.91%, test macro 53.49%\n",
            "Epoch 42: loss 0.06805, train micro 97.39%, train macro 86.49%, valid micro 88.41%, valid macro 51.67%, test micro 90.91%, test macro 58.11%\n",
            "Epoch 43: loss 0.06231, train micro 97.52%, train macro 87.62%, valid micro 87.2%, valid macro 50.91%, test micro 90.3%, test macro 57.42%\n",
            "Epoch 44: loss 0.05672, train micro 97.65%, train macro 88.67%, valid micro 87.2%, valid macro 54.53%, test micro 89.7%, test macro 56.77%\n",
            "Epoch 45: loss 0.05114, train micro 98.04%, train macro 90.66%, valid micro 87.2%, valid macro 54.53%, test micro 88.48%, test macro 55.6%\n",
            "Epoch 46: loss 0.04568, train micro 98.17%, train macro 91.56%, valid micro 86.59%, valid macro 54.05%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 47: loss 0.04038, train micro 98.31%, train macro 92.25%, valid micro 85.37%, valid macro 53.14%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 48: loss 0.03545, train micro 98.17%, train macro 91.74%, valid micro 85.37%, valid macro 53.14%, test micro 87.88%, test macro 58.25%\n",
            "Epoch 49: loss 0.03108, train micro 98.17%, train macro 92.22%, valid micro 84.76%, valid macro 52.72%, test micro 87.88%, test macro 58.25%\n",
            "Epoch 50: loss 0.02711, train micro 98.17%, train macro 92.22%, valid micro 83.54%, valid macro 51.91%, test micro 87.27%, test macro 57.65%\n",
            "Epoch 51: loss 0.02329, train micro 98.57%, train macro 94.06%, valid micro 84.15%, valid macro 52.3%, test micro 86.67%, test macro 57.07%\n",
            "Epoch 52: loss 0.01975, train micro 98.31%, train macro 93.24%, valid micro 84.76%, valid macro 52.72%, test micro 84.85%, test macro 55.5%\n",
            "Epoch 53: loss 0.01636, train micro 98.04%, train macro 92.47%, valid micro 84.76%, valid macro 52.72%, test micro 84.85%, test macro 55.5%\n",
            "Epoch 54: loss 0.01328, train micro 98.44%, train macro 93.81%, valid micro 84.15%, valid macro 52.3%, test micro 84.85%, test macro 57.91%\n",
            "Epoch 55: loss 0.01081, train micro 98.83%, train macro 95.23%, valid micro 84.15%, valid macro 52.3%, test micro 86.06%, test macro 59.06%\n",
            "Epoch 56: loss 0.00904, train micro 99.61%, train macro 98.35%, valid micro 84.76%, valid macro 52.72%, test micro 86.06%, test macro 59.06%\n",
            "Epoch 57: loss 0.00768, train micro 99.87%, train macro 99.44%, valid micro 85.37%, valid macro 53.14%, test micro 85.45%, test macro 58.47%\n",
            "Epoch 58: loss 0.00653, train micro 100.0%, train macro 100.0%, valid micro 86.59%, valid macro 54.05%, test micro 85.45%, test macro 58.47%\n",
            "Epoch 59: loss 0.00558, train micro 100.0%, train macro 100.0%, valid micro 86.59%, valid macro 54.05%, test micro 86.06%, test macro 59.06%\n",
            "Epoch 60: loss 0.00481, train micro 100.0%, train macro 100.0%, valid micro 86.59%, valid macro 54.05%, test micro 86.06%, test macro 56.52%\n",
            "Epoch 61: loss 0.00417, train micro 100.0%, train macro 100.0%, valid micro 87.2%, valid macro 54.53%, test micro 86.67%, test macro 57.07%\n",
            "Epoch 62: loss 0.00362, train micro 100.0%, train macro 100.0%, valid micro 86.59%, valid macro 54.05%, test micro 87.27%, test macro 57.65%\n",
            "Epoch 63: loss 0.00315, train micro 100.0%, train macro 100.0%, valid micro 85.98%, valid macro 50.2%, test micro 87.27%, test macro 57.65%\n",
            "Epoch 64: loss 0.00276, train micro 100.0%, train macro 100.0%, valid micro 85.37%, valid macro 46.05%, test micro 87.27%, test macro 57.65%\n",
            "Epoch 65: loss 0.00243, train micro 100.0%, train macro 100.0%, valid micro 84.76%, valid macro 45.87%, test micro 87.88%, test macro 58.25%\n",
            "Epoch 66: loss 0.00213, train micro 100.0%, train macro 100.0%, valid micro 84.76%, valid macro 45.87%, test micro 87.88%, test macro 58.25%\n",
            "Epoch 67: loss 0.00188, train micro 100.0%, train macro 100.0%, valid micro 84.76%, valid macro 45.87%, test micro 87.88%, test macro 58.25%\n",
            "Epoch 68: loss 0.00166, train micro 100.0%, train macro 100.0%, valid micro 84.76%, valid macro 45.87%, test micro 87.27%, test macro 57.65%\n",
            "Epoch 69: loss 0.00147, train micro 100.0%, train macro 100.0%, valid micro 84.15%, valid macro 45.7%, test micro 87.27%, test macro 57.65%\n",
            "Epoch 70: loss 0.00132, train micro 100.0%, train macro 100.0%, valid micro 84.15%, valid macro 45.7%, test micro 87.27%, test macro 57.65%\n",
            "Epoch 71: loss 0.00119, train micro 100.0%, train macro 100.0%, valid micro 84.15%, valid macro 45.7%, test micro 87.27%, test macro 57.65%\n",
            "Epoch 72: loss 0.00109, train micro 100.0%, train macro 100.0%, valid micro 84.15%, valid macro 45.7%, test micro 87.27%, test macro 57.65%\n",
            "Epoch 73: loss 0.001, train micro 100.0%, train macro 100.0%, valid micro 84.15%, valid macro 45.7%, test micro 87.27%, test macro 57.65%\n",
            "Epoch 74: loss 0.00092, train micro 100.0%, train macro 100.0%, valid micro 84.15%, valid macro 45.7%, test micro 88.48%, test macro 58.89%\n",
            "Epoch 75: loss 0.00085, train micro 100.0%, train macro 100.0%, valid micro 84.15%, valid macro 45.7%, test micro 88.48%, test macro 58.89%\n",
            "Epoch 76: loss 0.00078, train micro 100.0%, train macro 100.0%, valid micro 84.15%, valid macro 45.7%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 77: loss 0.00072, train micro 100.0%, train macro 100.0%, valid micro 84.15%, valid macro 45.7%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 78: loss 0.00066, train micro 100.0%, train macro 100.0%, valid micro 84.15%, valid macro 45.7%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 79: loss 0.00061, train micro 100.0%, train macro 100.0%, valid micro 84.76%, valid macro 45.87%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 80: loss 0.00057, train micro 100.0%, train macro 100.0%, valid micro 84.76%, valid macro 45.87%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 81: loss 0.00053, train micro 100.0%, train macro 100.0%, valid micro 84.76%, valid macro 45.87%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 82: loss 0.0005, train micro 100.0%, train macro 100.0%, valid micro 84.76%, valid macro 45.87%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 83: loss 0.00047, train micro 100.0%, train macro 100.0%, valid micro 84.76%, valid macro 45.87%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 84: loss 0.00045, train micro 100.0%, train macro 100.0%, valid micro 84.76%, valid macro 45.87%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 85: loss 0.00042, train micro 100.0%, train macro 100.0%, valid micro 84.76%, valid macro 45.87%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 86: loss 0.0004, train micro 100.0%, train macro 100.0%, valid micro 84.76%, valid macro 45.87%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 87: loss 0.00039, train micro 100.0%, train macro 100.0%, valid micro 84.76%, valid macro 45.87%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 88: loss 0.00037, train micro 100.0%, train macro 100.0%, valid micro 84.76%, valid macro 45.87%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 89: loss 0.00035, train micro 100.0%, train macro 100.0%, valid micro 84.76%, valid macro 45.87%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 90: loss 0.00034, train micro 100.0%, train macro 100.0%, valid micro 84.76%, valid macro 45.87%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 91: loss 0.00033, train micro 100.0%, train macro 100.0%, valid micro 84.76%, valid macro 45.87%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 92: loss 0.00032, train micro 100.0%, train macro 100.0%, valid micro 84.15%, valid macro 45.7%, test micro 89.09%, test macro 59.56%\n",
            "Epoch 93: loss 0.00031, train micro 100.0%, train macro 100.0%, valid micro 84.15%, valid macro 45.7%, test micro 88.48%, test macro 58.89%\n",
            "Epoch 94: loss 0.0003, train micro 100.0%, train macro 100.0%, valid micro 84.15%, valid macro 45.7%, test micro 88.48%, test macro 58.89%\n",
            "Epoch 95: loss 0.00029, train micro 100.0%, train macro 100.0%, valid micro 83.54%, valid macro 45.51%, test micro 87.88%, test macro 58.25%\n",
            "Epoch 96: loss 0.00028, train micro 100.0%, train macro 100.0%, valid micro 83.54%, valid macro 45.51%, test micro 87.27%, test macro 57.65%\n",
            "Epoch 97: loss 0.00027, train micro 100.0%, train macro 100.0%, valid micro 83.54%, valid macro 45.51%, test micro 87.27%, test macro 57.65%\n",
            "Epoch 98: loss 0.00026, train micro 100.0%, train macro 100.0%, valid micro 83.54%, valid macro 45.51%, test micro 87.27%, test macro 57.65%\n",
            "Epoch 99: loss 0.00025, train micro 100.0%, train macro 100.0%, valid micro 83.54%, valid macro 45.51%, test micro 87.27%, test macro 57.65%\n",
            "Epoch 100: loss 0.00025, train micro 100.0%, train macro 100.0%, valid micro 83.54%, valid macro 45.51%, test micro 87.27%, test macro 57.65%\n",
            "Best model: train micro 93.87%, train macro 48.42%, valid micro 91.46%, valid macro 47.77%, test micro 93.33%, test macro 48.28%\n"
          ]
        }
      ],
      "source": [
        "best_model = None\n",
        "best_val = 0\n",
        "\n",
        "output_size = hetero_graph.num_node_labels('paper')\n",
        "model = HeteroGNN(hetero_graph, args, aggr=\"attn\").to(args['device'])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
        "\n",
        "for epoch in range(args['epochs']):\n",
        "    loss = train(model, optimizer, hetero_graph, train_idx)\n",
        "    accs, best_model, best_val = test(model, hetero_graph, [train_idx, val_idx, test_idx], best_model, best_val)\n",
        "    print(\n",
        "        f\"Epoch {epoch + 1}: loss {round(loss, 5)}, \"\n",
        "        f\"train micro {round(accs[0][0] * 100, 2)}%, train macro {round(accs[0][1] * 100, 2)}%, \"\n",
        "        f\"valid micro {round(accs[1][0] * 100, 2)}%, valid macro {round(accs[1][1] * 100, 2)}%, \"\n",
        "        f\"test micro {round(accs[2][0] * 100, 2)}%, test macro {round(accs[2][1] * 100, 2)}%\"\n",
        "    )\n",
        "best_accs, _, _ = test(best_model, hetero_graph, [train_idx, val_idx, test_idx])\n",
        "print(\n",
        "    f\"Best model: \"\n",
        "    f\"train micro {round(best_accs[0][0] * 100, 2)}%, train macro {round(best_accs[0][1] * 100, 2)}%, \"\n",
        "    f\"valid micro {round(best_accs[1][0] * 100, 2)}%, valid macro {round(best_accs[1][1] * 100, 2)}%, \"\n",
        "    f\"test micro {round(best_accs[2][0] * 100, 2)}%, test macro {round(best_accs[2][1] * 100, 2)}%\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQgx5y4UqMHH"
      },
      "source": [
        "## Attention for each Message Type\n",
        "\n",
        "Through message type level attention we can learn that which message type is more important to which layer.\n",
        "\n",
        "Here we will print out and show that each layer pay how much attention on each message type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvK58gijqN_C",
        "outputId": "cf06ed81-fd52-4976-b445-343f87e25e31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 1 has attention 0.00885307788848877 on message type ('paper', 'author', 'paper')\n",
            "Layer 1 has attention 0.991146981716156 on message type ('paper', 'subject', 'paper')\n",
            "Layer 2 has attention 0.3925769329071045 on message type ('paper', 'author', 'paper')\n",
            "Layer 2 has attention 0.6074230670928955 on message type ('paper', 'subject', 'paper')\n"
          ]
        }
      ],
      "source": [
        "if model.convs1.alpha is not None and model.convs2.alpha is not None:\n",
        "    for idx, message_type in model.convs1.mapping.items():\n",
        "        print(f\"Layer 1 has attention {model.convs1.alpha[idx]} on message type {message_type}\")\n",
        "    for idx, message_type in model.convs2.mapping.items():\n",
        "        print(f\"Layer 2 has attention {model.convs2.alpha[idx]} on message type {message_type}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AGjsRqzAVri"
      },
      "source": [
        "More heterogeneous node classification examples please see the [examples/node_classification_hetero](https://github.com/snap-stanford/deepsnap/tree/master/examples/node_classification_hetero)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
